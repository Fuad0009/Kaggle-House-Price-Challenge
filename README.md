This is a machine learning problem. The provided dataset is a real life dataset of Ames Housing. There are 79 variables like alley type, land shape, proximity to various conditions, different facilities available etc. Using all these information we have to make a machine learning model which can predict the price of the given house with accuracy. 
There are several ways to do this. We can use Keras or high-level API available in Tensorflow to build a model but to show the understanding of the basic fundamentals of machine learning no such high-level API was used. Rather low-level API, library was used and model for the neural network was handmade to show proficiency. I did the coding in the whole project. 
The main difficulty in this project was not using high level APIs to get a good score and be in the top 10%. But this was overcome by cleaning the data thoroughly and using various techniques to get the desired result of 0.12 score in Kaggle competition.  
In this project an attempt was made to clean the data, analyzed it and fitted it to a model to predict the outcome. First the data was acquired and environment was created for working on the data. On the next step the data was explored and featured engineered. As there were many categorical data, I hot encoded the data and fitted a model to predict the house price. To explore the data and preprocess and feature engineer many things were done. They are given in sequence-
1.	Library was imported
2.	Outliers were removed 
3.	Skewness was checked and correlation analysis done. 
4.	Treating missing values by finding them and individually treating them. Depending on the variable and available data they were either filled with “None” or median was taken to fill the missing values or filled with most available values of that particular variable etc. 
Then I moved on the Modeling part. I used cross validation to calculate error. Then I applied different models to calculate error and finally take mean of all the models to get final score. I used LASSO(L1 regularization technique), kernel ridge(L2 regularization technique), elastic net regression, gradient boosting, xgb, lgb and all these were averaged to get a score of  0.10. I then calculated the cross-validation score of all the applied model and calculated the mean to get a score of 0.11. 
To implement neural network, we will need to calculate error. To calculate error, I first made mini-batches which were used to calculate and update error in gradient descent. Also, I needed to implement forward propagation and dropout in the model as dropout will improve accuracy. During the training I dropped out the nodes which have less activation score. Next, linear activation function was implemented which returned an output of the activation function. After that I built function to return the prediction and cost function to compute the cost. Here I used L2 regularization on the model to get the updated cost value of the network. Next, I built train, test and validation set for training the model. As saleprice is the value we want to predict so I separated “Saleprice” from the datasets. 
Lastly, I created the neural network model with hidden layers, neurons, learning rate, hidden layer activation function and optimizers and they were all tweakable as to get better result I may have to tweak them again and again to find out which set of parameters yields better results. This model was made so that it can train batch by batch using the minibatches created beforehand. I used 2 hidden layers and 310 hidden neurons. In dropout, I used 0.65 as probability of keeping the neuron. For the activation function I used “leaky-relu” for better performance. I used 1200 epochs and a changing learning rate. For optimizer I used “RMS-prop”. 
To understand how my model was performing I used graphs and rmsle (root mean squared logarithmic error). Grid search was implemented to find out which parameters are the best suited for best training result. In result I got the combination of the parameters and what their score is and from there I chose the parameters which gave the best result if implemented in training. By using the output of grid search I was able to minimize the validation loss. Then I applied grid search again on hyperparameters to get further better result. To reduce the score to .011 was the most challenging and the technique which helped me most was ensembling. By ensembling the models I was able to get a score of 0.12. 
The biggest learning from making this model was that ensembling can help a lot to reduce the error. Stacking one models on another can make the final model powerful.
